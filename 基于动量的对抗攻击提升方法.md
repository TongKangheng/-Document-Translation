<h1 align="center">基于动量的对抗攻击提升方法</h1>

$$
\begin{gather}
Yinpeng \space Dong^1, Fangzhou \space Liao^1, Tianyu \space Pang^1, Hang \space Su^1, Jun \space Zhu^{1*}, Xiaolin \space Hu^1, Jianguo \space Li^2\\
{}^1Department \space of \space Computer \space Science \space and \space Technology, Tsinghua \space Lab \space of \space Brain \space and \space Intelligence\\
{}^1Beijing \space National \space Research \space Center \space for \space Information \space Science \space and \space Technology, BNRist \space Lab\\
{}^2Intel \space Labs \space China\\
\{dyp17, liaofz13, pty17\}@mails.tsinghua.edu.cn,\{suhangss,dcszj,xlhu\}@mail.tsinghua.edu.cn,jianguo.li@intel.com
\end{gather}
$$

<h1 align="center">摘要</h1>

​		当面对对抗样本时，深度神经网络是脆弱的，潜在的严重后果给这些算法造成了安全问题。在深度学习模型部署之前，对抗攻击是一种评估它们鲁棒性的重要代表。然而，大多数已有的对抗攻击只能以较低的成功率欺骗黑盒模型。为了解决这个问题，我们提出了一大类基于动量的迭代算法来增强对抗攻击。通过将动量项整合到攻击的迭代过程中，我们的方法可以稳定更新方向，并在迭代过程中摆脱不良的局部最大值，从而产生更具可移植性的对抗样本。为了进一步提高黑盒攻击的成功率，我们将动量迭代算法应用于一组模型，并证明经过对抗训练的、具有强大防御能力的模型也容易受到我们的黑盒攻击。我们希望所提出的方法将作为评估各种深度模型和防御方法的鲁棒性的基准。通过这种方法，我们在NIPS 2017非针对性对抗攻击和针对性对抗攻击比赛中获得了第一名。

# 1、介绍

​		由于面对对抗样本[[23](#23)，[5](#5)]时的脆弱性，深度神经网络（DNN）受到了挑战，这些样本是通过向合法样本中添加人类无法察觉的细微噪声来制作的，但它们会为模型输出提供攻击者所需的不准确预测。对抗样本的生成获得了越来越多的关注，因为它有助于在启动模型之前确定其脆弱性。此外，对抗样本还通过提供更多样化的训练数据[[5](#5)，[10](#10)]来促进各种DNN算法获得鲁棒性。

​		知道了给定模型的结构和参数，许多方法都能以白盒方式成功生成对抗样本，包括基于优化的方法，例如盒约束L-BFGS [[23](#23)]，基于单步梯度的方法，例如快速梯度符号[[5](#5)]和基于梯度的方法的迭代变体[[9](#9)]。通常，对抗样本的一个更严重的问题是它们的良好可移植性[[23](#23)，[12](#12)，[14](#14)]，即为一种模型设计的对抗样本对于其他模型仍然具有对抗性，因此使黑盒攻击在实际应用中切实可行并构成真正的安全问题。可移植性的现象是由于不同的机器学习模型在一个数据点周围学习了类似的决策边界这一事实，使得为一种模型设计的对抗样本对其他模型有效。

​		但是，现有的攻击方法在攻击黑盒模型时效率低下，尤其是对于具有防御机制的模型。例如，整体对抗训练[24]显著提高了深度神经网络的鲁棒性，并且大多数现有方法无法以黑盒方式成功攻击它们。这个事实很大程度上归因于攻击能力和可移植性之间的权衡。特别是，通过基于优化的迭代方法生成的对抗样本具有较差的可移植性[10](#10)，因此使黑盒攻击的有效性降低。另一方面，基于梯度的单步方法会产生更加可移植的对抗样本，但是对于白盒模型而言，它们的成功率通常较低[10](#10)，从而使其对黑盒攻击无效。考虑到实际黑盒攻击的困难，Papernot等人 [[16](#16)]使用自适应查询来训练替代模型以完全表征目标模型的行为，从而将黑盒攻击转变为白盒攻击。但是，它需要目标模型给出的完全预测置信度和大量查询，尤其是对于大型数据集（例如ImageNet [[19](#19)]）。这样的要求在实际应用中是不切实际的。因此，我们考虑如何在不知道其架构和参数的情况下，有效地攻击黑盒模型，而且无需查询。

​		在本文中，我们提出了一系列基于动量迭代梯度的方法，以提高生成的对抗样本的成功率。除了基于迭代梯度的方法，该方法使用梯度来迭代扰动输入以最大化损失函数[[5](#5)]，基于动量的方法还可以在迭代过程中沿损失函数的梯度方向累积速度矢量，目的是稳定更新方向和从不良的局部最大值中逃脱。我们表明，由动量迭代方法生成的对抗样本在白盒和黑盒攻击中均具有较高的成功率。所提出的方法减轻了白盒攻击与可传递性之间的折衷，并且比单步方法[[5](#5)]和香草迭代方法[[9](#9)]发挥了更强的攻击算法的作用。

​		为了进一步提高对抗样本的可移植性，我们研究了几种攻击一组模型的方法，因为如果一个对抗样本欺骗了多个模型，则对于其他黑盒模型更可能保持对抗性[[12](#12)]。我们表明在黑盒情况下，由动量迭代方法为多个模型生成的对抗样本可以成功地欺骗以整体对抗训练[[24](#24)]所获得的鲁棒模型。本文的研究结果为开发更鲁棒的深度学习模型提出了新的安全问题，希望我们的攻击将被用于评估各种深度学习模型和防御方法的鲁棒性的基准。总之，我们做出以下贡献：

* 我们引入一类被称为基于动量梯度迭代的攻击算法，其中，我们在每次迭代时都会累积损失函数的梯度，以稳定优化并摆脱不良的局部最大值。
* 我们研究了几种同时攻击多个模型的整体方法，通过保持很高的攻击成功率，证明了强大的可移植性。
* 我们首次证明了，通过整体对抗训练获得的、具有强大防御能力的模型面对黑盒攻击也是脆弱的。

# 2、背景

​		在本节中，我们提供了背景知识，并回顾了对抗性攻击和防御方法的相关工作。给定一个分类器$f(\pmb{x}):\pmb{x}\in\mathcal{\pmb{x}}\rightarrow y\in \mathcal{Y}$，它输出一个标号$y$作为对输入$\pmb{x}$的预测，对抗性攻击的目标是在$\pmb{x}$附近寻找一个样本$\pmb{x}^*$，但是被分类器分类错误。具体来说，有两类对抗样本：非目标和目标。对于一个具有真实标签的、正确分类的输入$\pmb{x}$，比如$f(\pmb{x})=y$，一个非目标对抗样本$\pmb{x}^*$是通过给$\pmb{x}$添加小噪声而不改变标签来制作的，但误导分类器以至于$f(\pmb{x}^*)\neq y$；而一个目标对抗样本旨在通过输出一个特定标签$f(\pmb{x}^*)=y^*$来愚弄分类器，其中$y^*$标签是攻击者指定的目标标签，并且$y^*\neq y$。在大多数情况下，敌方噪声的峰值$L_p$必须小于允许值$\epsilon$，$||\pmb{x}^*-\pmb{x}||_p\le\epsilon$，其中$p$应为$0,1,2,\infin$。

## 2.1、攻击方法

​		现有的生成对抗样本的方法可以分为三类。我们在这里介绍他们的非目标版本的攻击，目标版本可以简单地得出。

​		**基于梯度的单步方法**，如快速梯度符号法（FGSM）[[5](#5)]，通过最大化损失函数$J(\pmb{x}^*,y)$找到一个对抗样本$\pmb{x}^*$，其中$J$通常是交叉熵损失。FGSM通过下面的公式，生成满足$L_\infin$范数$||\pmb{x}^*-\pmb{x}||_\infin \le \epsilon$的对抗样本：
$$
\pmb{x}^*=\pmb{x}+\epsilon \cdot sign(\nabla_{\pmb{x}}J(\pmb{x},y) \tag 1
$$
其中$\nabla_{\pmb{x}}J(\pmb{x},y)$是损失函数关于$\pmb{x}$的梯度。快速梯度法（FGM）是FGSM的一个特例，它满足$L_2$范数界$||\pmb{x}^*-\pmb{x}||_2 \le \epsilon$，即：
$$
\pmb{x}^*=\pmb{x}+\epsilon \cdot \frac{\nabla_{\pmb{x}}J(\pmb{x},y)}{||\nabla_{\pmb{x}}J(\pmb{x},y)||_2} \tag 2
$$
​		**迭代方法**[[9](#9)]以小步长$\alpha$迭代地多次应用快速梯度。 FGSM的迭代版本（I-FGSM）可以表示为：
$$
\pmb{x}_0^*=\pmb{x},\pmb{x}^*_{t+1}=\pmb{x}^*_t+\alpha \cdot sign(\nabla_{\pmb{x}}J(\pmb{x}^*_t,y)) \tag 3
$$
为了使生成的对抗样本满足$L_\infin$（或者$L_2$）界线，可以将$\pmb{x}^*_t$裁剪到$\pmb{x}$的附近（以$\epsilon$为参数），或者简单地设置$\alpha=\epsilon/T$，T为迭代次数。在白盒情况下，迭代方法展现了比单步方法更加强大的对抗性，但是缺点是以可移植性较差为代价[[10](#10)，[24](#24)]。

​		**基于优化的方法**[[23](#23)]直接优化了真实样本与对抗样本之间的距离，但会导致对抗样本分类错误。盒约束的L-BFGS可以用来解决这个问题。一种更复杂的方法[[1](#1)]是解下面这个式子：
$$
\mathop{\arg\min}_{\pmb{x}^*}||\pmb{x}^*-\pmb{x}||_p-J(\pmb{x}^*,y) \tag 4
$$
由于它直接优化了对抗样本与相应真实样本之间的距离，因此无法保证$L_\infin$（$L_2$）距离小于所需值。与迭代方法一样，基于优化的方法也缺乏黑盒攻击的功效。

## 2.2防御方法

​		在许多尝试[[13](#13)、[3](#3)、[15](#15)、[10](#10)、[24](#24)、[17](#17)、[11](#11)]中，对抗训练是提高DNN鲁棒性的、收到最广泛研究的方法[[5](#5)、[10](#10)、[24](#24)]。通过将对抗样本注入训练过程中，经过对抗性训练的模型将学会抵抗损失函数的梯度方向上的扰动。但是，由于生成对抗样本和所训练的参数的耦合，它们没有获得面对黑盒攻击的鲁棒性。集成对抗训练[24](#24)利用不仅仅是受训练模型中，而且还有其他保持模型中产生的对抗样本来扩充训练数据。因此，经过整体对抗训练的模型对于单步攻击和黑盒攻击具有鲁棒性。

# 3、方法论

​		在本文中，我们提出了一系列**基于动量迭代梯度的方法**来生成对抗样本，这些样本可以欺骗白盒模型和黑盒模型。在本节中，我们将详细阐述所提出的算法。我们首先说明如何将动量集成到迭代FGSM中，该方法引入了动量迭代快速梯度符号方法（MI-FGSM），以非目标攻击方式生成满足$L_\infin$范数限制的对抗样本。然后，我们提出了几种有关如何有效攻击模型集成的方法。最后，我们将MI-FGSM扩展到L2范式绑定和有针对性的攻击，从而产生了广泛的攻击方法。

---

**算法 1** MI-FGSM

---

**输入**：一个以$J$为损失函数的分类器$f$ ；真实样本$\pmb{x}$和真实标签$y$；

**输入**:扰动大小$\epsilon$；迭代次数$T$和衰减因子$\mu$。

**输出**:一个满足$||\pmb{x}^*-\pmb{x}||_\infin \le \epsilon$的对抗样本$\pmb{x}^*$。

​	1: $\alpha=\epsilon/T$；

​	2: $\pmb{g}_0=0; \pmb{x}_0^*=\pmb{x}$；

​	3: **for** $t=0$ to $T-1$ **do**

​	4: 		输入$\pmb{x}^*_t$给$f$并且获取梯度$\nabla_{\pmb{x}}J(\pmb{x}^*_t,y)$;

​	5:		 通过累积速度矢量到梯度方向上更新 $\pmb{g}_{t+1}$：
$$
\pmb{g}_{t+1}=\mu \cdot \pmb{g}_t+\frac{\nabla_{\pmb{x}}J(\pmb{x}_t^*,y)}{||\nabla_{\pmb{x}}J(\pmb{x}_t^*,y)||_1}; \tag 6
$$
​	6:		 利用梯度方向更新$\pmb{x}^*_{t+1}$：
$$
\pmb{x}^*_{t+1} = \pmb{x}^*_{t}+\alpha \cdot sign(\pmb{g}_{t+1}); \tag 7
$$


​	7: **end for**

​	8: **return** $\pmb{x}^*=\pmb{x}_T^*$.

---

## 3.1、动量迭代快速梯度符号法

​		动量法[[18](#18)]是一种通过在迭代过程中沿损失函数的梯度方向累积速度矢量来加速梯度下降算法的技术。以前的梯度的记忆有助于穿越狭窄的山谷，小驼峰和较差的局部最小值或最大值[[4](#4)]。动量法还显示了其在随机梯度下降中稳定更新的有效性[[20](#20)]。我们运用动量的思想来产生对抗性的例子并获得巨大的效益。

​		为了从满足$L_\infin$范数界的真实样本$\pmb{x}$生成非目标对抗样本$\pmb{x}^*$，基于梯度的方法通过解决约束优化问题来寻找对抗样本：
$$
\mathop{\arg\max}_{\pmb{x}^*}J(\pmb{x}^*,y),\space s.t. \space ||\pmb{x}^*-\pmb{x}||_\infin\le \epsilon, \tag 5
$$
其中$\epsilon$是对抗性扰动的大小。 FGSM通过在数据点周围决策边界的线性假设下，仅一次将梯度的符号应用于真实样本（在等式（1）中）来生成一个对抗示例。但是在实践中，当扰动较大时，线性假设可能不成立[[12](#12)]，这使得FGSM生成的对抗样本“欠拟合”模型，从而限制了其攻击能力。相反，（在等式（3）中），在每次迭代中，迭代FGSM贪婪地将对抗性示例沿梯度的符号方向移动。因此，对抗样本很容易陷入不良的局部最大值并“过拟合”模型，使其不太可能在模型之间转移。

​		为了克服这种难题，我们将动量整合到迭代FGSM中，以稳定更新方向并摆脱出现的不良的局部最大值。因此，当增加迭代次数时，基于动量的方法仍可保持对抗样本的可传递性，同时，对于像迭代FGSM这样的白盒模型，它也可作为强大的对手。它减轻了攻击能力和可传递性之间的折衷，展现了强大的黑盒攻击能力。

​		算法1中总结了动量迭代快速梯度符号方法（MI-FGSM）。具体而言，$\pmb{g}_t$使用公式（6）中定义的衰减因子$\mu$，收集前$t$次迭代的梯度。然后，在公式（7）中，对抗样本$\pmb{x}_t^*$直到第$t$次迭代在$\pmb{g}_t$的符号方向上被扰动，步长为$\alpha$。如果$\mu$等于0，则MI-FGSM退化为迭代FGSM。在每次迭代中，当前梯度$\nabla_{\pmb{x}}J(\pmb{x}_t^*,y)$通过其自身的$L_1$距离（任何距离度量都是可行的）进行归一化，因为我们注意到不同迭代中梯度的大小有所不同。

## 3.2、攻击模型集合

​		在本节中，我们研究如何有效地攻击模型集合。集成方法已被广泛用于研究和竞赛中，以提高性能和鲁棒性[[6](#6)、[8](#8)、[2](#2)]。集合的思想也可以应用于对抗攻击，因为这样的事实：如果一个样本对多个模型仍然具有对抗性，那么它可能会捕获一个固有的方向，该方向总是欺骗这些模型，并且更有可能同时移植到其他模型[ [12](#12)]，从而启用强大的黑盒攻击。

​		我们准备攻击将logits函数（logits函数是softmax的输入值，一般是全连接层的输出，也可以用来表征概率，只是还没有进行归一化，由于logits没有一个标准的中文名，因此这里仍然沿用其英文，下同）融合在一起的多个模型，并将此方法称为*logits集合*。因为logits捕获了概率预测之间的对数关系，所以由logits融合的模型集合会汇总所有模型的精细详细输出，可以轻松发现其漏洞。特别是，为了攻击K个模型的集合，我们将logits融合为：
$$
\pmb{l}(\pmb{x})=\sum_{k=1}^Kw_k\pmb{l}_k(\pmb{x}) \tag 8
$$
其中$\pmb{l}_k(\pmb{x})$是第$k$个模型的logits，$w_k$是满足$s_k\ge0$且$\sum_{k=1}^Kw_k=1$的整体权重。损失函数$J(\pmb{x},y)$定义为软最大交叉熵损失：
$$
J(\pmb{x},y)=-\pmb{1}_y\cdot log(softmax(\pmb{l}(\pmb{x}))),\tag 9
$$
其中$\pmb{1}_y$是$y$的独热编码。我们总结了MI-FGSM算法，在算法2中用于攻击平均了多个logits的模型。

​		为了进行比较，我们还介绍了两种替代的集成方案，其中一个已经进行了研究[[12](#12)]。具体而言，可以在预测中对$K$个模型平均为[[12](#12)]$\pmb{p}(\pmb{x})=\sum_{k=1}^Kw_k\pmb{p}_k(\pmb{x})$，其中$\pmb{p}_k(\pmb{x})$是给出输入$\pmb{x}$下第$k$个模型的预测概率。$K$个模型的损失也可以平均为$J(\pmb{x},y)=\sum_{k=1}^Kw_kJ_k(x,y)$。

---

**算法 2** 针对模型集合的MI-FGSM

---

**输入：**K个分类器的logits $\pmb{l}_1,\pmb{l}_2,……,\pmb{l}_k$；集合权重$w_1,w_2,……,w_K$；一个真实样本$\pmb{x}$和真实标签$y$；

**输入：**扰动大小$\epsilon$；迭代次数$T$和衰减因子$\mu$。

**输出：**一个对抗样本$\pmb{x}^*$，满足$||\pmb{x}^*-\pmb{x}||_\infin \le \epsilon$。

1: $\alpha=\epsilon /T %$；

2: $\pmb{g}_0=0;\pmb{x}_0^*=\pmb{x}$；

3: **for** $t=0$ to $T-1$ **do**

4: 	输入$\pmb{x}_t^*$并且输出$\pmb{l}(\pmb{x}_t^*)$，对$k=1,2,……,K$；

5: 	融合logits为$\pmb{l}(\pmb{x}^*_t)=\sum_{k=1}^Kw_k\pmb{l}_k(\pmb{x}_t^*)$；

6: 	根据$\pmb{l}(\pmb{x}_t^*)$和公式（9）获取软最大交叉熵损失$J(\pmb{x}^*_t,y)$；

7: 	得到梯度$\nabla_{\pmb{x}}J(\pmb{x}_t^*,y)$；

8: 	根据公式（6）更新$\pmb{g}_{t+1}$；

9: 	根据公式（7）更新$\pmb{x}_{t+1}^*$；

10: **end for**

11: **return** $\pmb{x}^*=\pmb{x}_T^*$。

---

在这三种方法中，唯一的区别是在哪里组合多个模型的输出，但它们会导致不同的攻击能力。我们根据经验经验发现，在集合的各种攻击方法和各种模型中，logits集合在预测和损失方面的性能都优于预测集合和损失集合，这将在4.3.1节中加以说明。

## 3.3、拓展

​		动量迭代方法可以很容易地推广到其他攻击环境。通过用前面所有步骤的累积梯度代替当前梯度，任何迭代方法都可以推广到其动量变体。这里本文从$L_2$范数界限攻击和目标攻击两个方面介绍了生成对抗样本的方法。

​		为了在距离为$||\pmb{x}^*-\pmb{x}||\le\epsilon$的区域样本附近找到对抗样本，迭代快速梯度法（MI-FGM）的动量变量可以写成
$$
\pmb{x}_{t+1}^*=\pmb{x}_t^*+\alpha\cdot\frac{\pmb{g}_{t+1}}{||\pmb{g}_{t+1}||_2},\tag {10}
$$
这里$\pmb{g}_{t+1}$在公式（6）中定义，并且$\alpha=\epsilon /T$，T代表迭代总次数。

​		对于目标攻击，寻找误分类为目标类$y^*$的典型例子的目的是最小化损失函数$J(\pmb{x}^*,y^*)$。累积梯度的计算公式为
$$
\pmb{g}_{t+1}=\mu \cdot \pmb{g}_t+\frac{J(\pmb{x}_t^*,y^*)}{||\nabla_{\pmb{x}}J(\pmb{x}_t^*,y^*)||_1}; \tag{11}
$$
以$L_\infin$范数为界的目标MI_FGSM是：
$$
\pmb{x}_{t+1}^*=\pmb{x}_t^*-\alpha\cdot sign(\pmb{g}_{t+1}) \tag{12}
$$
并且以$L_2$范数为界线的目标MI_FGM是：
$$
\pmb{x}_{t+1}^*=\pmb{x}_t^*-\alpha \cdot \frac{\pmb{g}_{t+1}}{||\pmb{g}_{t+1}||_2}\tag{13}
$$
因此，我们介绍了一系列的动量迭代方法来解决各种情况下的攻击，其有效性在第4节中得到了证明。

# 4、实验

​		在本节中，我们在ImageNet数据集[[19](#19)]上进行了大量实验，以验证所提出方法的有效性。我们首先在4.1节中确定实验设置。 然后我们在4.2节中报告了对单个模型的测试结果，在4.3节中报告了一组模型的测试结果我们的方法赢得了NIPS 2017非目标和目标对抗攻击竞赛，配置介绍见4.4节。

## 4.1、设置

​		我们研究了七个模型，其中四个是正常训练模型（Inc-v3）[[22](#22)]、（Inc-v4）、（IncRes-v2）[[21](#21)]、（Res-152）[[7](#7)]，另外三个模型由集成对抗训练$Inc-v3_{ens3}$、$Inc-v3_{ens4}$、$IncRes-v2_{ens}$[[24](#24)]。我们将最后三个模型简单地称为“对抗训练模型”，没有歧义。

​		如果模型不能对原始图像进行分类，那么研究攻击的成功率就没有什么意义了没错，所以，我们从ILSVRC 2012验证集中随机选择了1000张属于1000个类别的图像，它们都被正确分类。

​		在我们的实验中，我们将我们的方法与单步梯度法和迭代法进行了比较。由于基于优化的方法不能显式地控制对抗样本和相应的真实样本之间的距离，因此它们不能直接与我们的方法相比，但是它们与第2.1节中讨论的迭代方法具有相似的性质。为清晰起见，我们只报告了基于非目标攻击的$L_\infin$范数界的结果，并在补充资料中留下了基于$L_2$范数界和目标攻击的结果。本文的研究结果在不同的攻击环境下具有普遍性。

## 4.2、攻击单个模型

​		我们在表1中报告了我们所考虑的模型的攻击成功率。Inc-v3、Inc-v4、InvRes-v2和Res-152分别采用FGSM、迭代FGSM（I-FGSM）和MI-FGSM攻击方法生成了对抗样本。成功率是以对抗样本为输入的相应模型的误分类率。在所有实验中，最大扰动$\epsilon$被设置为16，像素值在[0，255]中。I-FGSM和MI-FGSM的迭代次数为10次，衰减因子$\mu$为1.0，这将在4.2.1节中研究。

​		从表中可以看出，MI-FGSM仍然像I-FGSM一样是强大的白盒对手，因为它可以以接近100％的成功率攻击白盒模型。另一方面，可以看出，与单步FGSM相比，I-FGSM降低了黑盒攻击的成功率。但是，通过整合动量，我们的MI-FGSM在黑盒攻击中的性能明显优于FGSM和I-FGSM。在大多数黑盒攻击案例中，它的成功率是I-FGSM的2倍以上，证明了所提算法的有效性。我们在图1中显示了为Inc-v3生成的两个对抗图像。

​		应该注意的是，尽管我们的方法大大提高了黑盒攻击的成功率，但是对于以黑盒方式攻击经过对抗训练的模型（例如，对于$IncRes-v2_{ens}$而言不足16％）仍然无效。后来我们证明了基于集成的方法极大地改善了4.3节中的结果。接下来，我们研究MI-FGSM不同于原始迭代方法的几个方面，以进一步解释为什么它在实践中表现良好。

### 4.2.1、衰减因子$\mu$

​		衰减因子$\mu$在提高攻击成功率方面起着关键作用。如果$\mu=0$，则基于动量的迭代方法简单地转向普通的迭代方法。因此，我们研究了适当的衰减因子值。我们用MI-FGSM攻击Inc-v3模型，扰动大小$\epsilon=16$，迭代次数10，衰减因子范围从0.0到2.0，粒度为0.1。图2中针对Inc-v3，Inc-v4，IncRes-v2和Res-152生成的对抗样本的成功率。成功率对黑盒模型的曲线是单峰的，其最大值在$\mu=1.0$左右获得。当$\mu=1.0$时，对等式（6）中定义的$\pmb{g}_t$的另一种解释只是将所有以前的提督加起来以执行当前更新。

### 4.2.2、迭代次数

​		然后，我们研究使用I-FGSM和MI-FGSM时迭代次数对成功率的影响。我们采用相同的超参数（即$\epsilon=16,\mu=1.0$）来攻击Inc-v3模型，迭代次数为1到10，然后评估针对Inc-v3，Inc-v4，IncRes-v2和Res-152模型的对抗样本的成功率，结果如图3所示。

​		可以看到，当增加迭代次数时，I-FGSM相对于黑盒模型的成功率逐渐降低，而MI-FGSM的成功率则保持较高的值。结果证明了我们的论点，即通过迭代方法生成的对抗示例很容易适合白盒模型，但不太可能在模型之间转移。但是基于动量的迭代方法有助于减轻白盒攻击和可传递性之间的折衷，从而同时展示了针对白盒和黑盒模型的强大攻击能力。

### 4.2.3、更新方向

​		为了解释为什么MI-FGSM具有更好的移植能力，我们进一步研究了I-FGSM和MI-FGSM在迭代过程中给出的更新方向。我们计算两个连续扰动的余弦相似度，并在攻击Inc-v3时在图4中显示结果。由于MI-FGSM中的余弦相似度值较大，因此MI-FGSM的更新方向比I-FGSM的更新方向更稳定。

​		回想一下，可移植性来自以下事实：模型在数据点周围学习相似的决策边界[[12](#12)]。尽管决策边界是相似的，但由于DNN的高度非线性结构，它们不可能相同。因此，模型的数据点周围可能存在一些例外的决策区域（[[12](#12)]中的图4和图5所示的漏洞），这些决策区域很难转移到其他模型。这些区域在优化过程中对应于较差的局部最大值，并且迭代方法可以轻松地陷入此类区域，从而导致可移植的对抗样本较少。另一方面，通过图4所示的动量方法获得的稳定更新方向可以帮助从这些例外区域中逃脱，从而为对抗性攻击提供更好的可传递性。另一种解释是，稳定的更新方向使摄动的$L_2$范数更大，这可能对可移植性有所帮助。

### 4.2.4、扰动大小

​		我们最终研究了对抗扰动的大小对成功率的影响。我们使用图像强度[0,255]的FGSM，I-FGSM和MI-FGSM，$\epsilon$从1到40范围攻击Inc-v3模型，并评估在白盒模型Inc-v3和黑盒模型Res-152上的性能。在我们的实验中，我们将I-FGSM和MI-FGSM的步长$\alpha$设置为1，因此迭代次数随扰动的大小线性增长。结果如图5所示。

​		对于白盒攻击，迭代方法很快会达到100％的成功率，但是当扰动较大时，单步FGSM的成功率会降低。当扰动较大时，这种现象很大程度上归因于决策边界为线性的不恰当假设[[12](#12)]。对于黑盒攻击，尽管这三种方法的成功率随扰动的大小线性增长，但MI-FGSM的成功率增长更快。换句话说，为了攻击具有成功率的黑盒模型，MI-FGSM可以使用较小的扰动，这在视觉上对于人类来说是无法区分的。

## 4.3 攻击集成模型

​		在本节中，我们显示了攻击一系列模型的实验结果。我们首先比较3.2节中介绍的三种集成方法。然后证明经过对抗训练的模型容易受到我们的黑盒攻击。

### 4.3.1、几种集成模型的比较

​		在本节中，我们将对攻击的整体方法进行比较。我们在研究中包括四个模型，分别是Inc-v3，Inc-v4，IncRes-v2和Res-152。在我们的实验中，我们将一个模型保留为黑盒模型，并分别通过FGSM，I-FGSM和MI-FGSM攻击其他三个模型的集合，以全面比较这三个集合方法的结果，即logits集合，预测集合和损失集合。我们将I-FGSM和MI-FGSM的迭代次数设置为16，将MI-FGSM的迭代次数设置为10，$\mu$设置为1.0，并将集合权重均等。结果示于表2。









<span id="5">[5]</span>