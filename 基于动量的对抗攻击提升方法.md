<h1 align="center">基于动量的对抗攻击提升方法</h1>

$$
\begin{gather}
Yinpeng \space Dong^1, Fangzhou \space Liao^1, Tianyu \space Pang^1, Hang \space Su^1, Jun \space Zhu^{1*}, Xiaolin \space Hu^1, Jianguo \space Li^2\\
{}^1Department \space of \space Computer \space Science \space and \space Technology, Tsinghua \space Lab \space of \space Brain \space and \space Intelligence\\
{}^1Beijing \space National \space Research \space Center \space for \space Information \space Science \space and \space Technology, BNRist \space Lab\\
{}^2Intel \space Labs \space China\\
\{dyp17, liaofz13, pty17\}@mails.tsinghua.edu.cn,\\
\{suhangss,dcszj,xlhu\}@mail.tsinghua.edu.cn,jianguo.li@intel.com
\end{gather}
$$

<h1 align="center">摘要</h1>

​		当面对对抗样本时，深度神经网络是脆弱的，潜在的严重后果给这些算法造成了安全问题。在深度学习模型部署之前，对抗攻击是一种评估它们鲁棒性的重要代表。然而，大多数已有的对抗攻击只能以较低的成功率欺骗黑盒模型。为了解决这个问题，我们提出了一大类基于动量的迭代算法来增强对抗攻击。通过将动量项整合到攻击的迭代过程中，我们的方法可以稳定更新方向，并在迭代过程中摆脱不良的局部最大值，从而产生更具可移植性的对抗样本。为了进一步提高黑盒攻击的成功率，我们将动量迭代算法应用于一组模型，并证明经过对抗训练的、具有强大防御能力的模型也容易受到我们的黑盒攻击。我们希望所提出的方法将作为评估各种深度模型和防御方法的鲁棒性的基准。通过这种方法，我们在NIPS 2017非针对性对抗攻击和针对性对抗攻击比赛中获得了第一名。

# 1、介绍

​		由于面对对抗样本[[23](#23)，[5](#5)]时的脆弱性，深度神经网络（DNN）受到了挑战，这些样本是通过向合法样本中添加人类无法察觉的细微噪声来制作的，但它们会为模型输出提供攻击者所需的不准确预测。对抗样本的生成获得了越来越多的关注，因为它有助于在启动模型之前确定其脆弱性。此外，对抗样本还通过提供更多样化的训练数据[[5](#5)，[10](#10)]来促进各种DNN算法获得鲁棒性。

​		知道了给定模型的结构和参数，许多方法都能以白盒方式成功生成对抗样本，包括基于优化的方法，例如盒约束L-BFGS [[23](#23)]，基于单步梯度的方法，例如快速梯度符号[[5](#5)]和基于梯度的方法的迭代变体[[9](#9)]。通常，对抗样本的一个更严重的问题是它们的良好可移植性[[23](#23)，[12](#12)，[14](#14)]，即为一种模型设计的对抗样本对于其他模型仍然具有对抗性，因此使黑盒攻击在实际应用中切实可行并构成真正的安全问题。可移植性的现象是由于不同的机器学习模型在一个数据点周围学习了类似的决策边界这一事实，使得为一种模型设计的对抗样本对其他模型有效。

​		但是，现有的攻击方法在攻击黑盒模型时效率低下，尤其是对于具有防御机制的模型。例如，整体对抗训练[24]显著提高了深度神经网络的鲁棒性，并且大多数现有方法无法以黑盒方式成功攻击它们。这个事实很大程度上归因于攻击能力和可移植性之间的权衡。特别是，通过基于优化的迭代方法生成的对抗样本具有较差的可移植性[10](#10)，因此使黑盒攻击的有效性降低。另一方面，基于梯度的单步方法会产生更加可移植的对抗样本，但是对于白盒模型而言，它们的成功率通常较低[10](#10)，从而使其对黑盒攻击无效。考虑到实际黑盒攻击的困难，Papernot等人 [[16](#16)]使用自适应查询来训练替代模型以完全表征目标模型的行为，从而将黑盒攻击转变为白盒攻击。但是，它需要目标模型给出的完全预测置信度和大量查询，尤其是对于大型数据集（例如ImageNet [[19](#19)]）。这样的要求在实际应用中是不切实际的。因此，我们考虑如何在不知道其架构和参数的情况下，有效地攻击黑盒模型，而且无需查询。

​		在本文中，我们提出了一系列基于动量迭代梯度的方法，以提高生成的对抗样本的成功率。除了基于迭代梯度的方法，该方法使用梯度来迭代扰动输入以最大化损失函数[[5](#5)]，基于动量的方法还可以在迭代过程中沿损失函数的梯度方向累积速度矢量，目的是稳定更新方向和从不良的局部最大值中逃脱。我们表明，由动量迭代方法生成的对抗样本在白盒和黑盒攻击中均具有较高的成功率。所提出的方法减轻了白盒攻击与可传递性之间的折衷，并且比单步方法[[5](#5)]和香草迭代方法[[9](#9)]发挥了更强的攻击算法的作用。

​		为了进一步提高对抗样本的可移植性，我们研究了几种攻击一组模型的方法，因为如果一个对抗样本欺骗了多个模型，则对于其他黑盒模型更可能保持对抗性[[12](#12)]。我们表明在黑盒情况下，由动量迭代方法为多个模型生成的对抗样本可以成功地欺骗以整体对抗训练[[24](#24)]所获得的鲁棒模型。本文的研究结果为开发更鲁棒的深度学习模型提出了新的安全问题，希望我们的攻击将被用于评估各种深度学习模型和防御方法的鲁棒性的基准。总之，我们做出以下贡献：

* 我们引入一类被称为基于动量梯度迭代的攻击算法，其中，我们在每次迭代时都会累积损失函数的梯度，以稳定优化并摆脱不良的局部最大值。
* 我们研究了几种同时攻击多个模型的整体方法，通过保持很高的攻击成功率，证明了强大的可移植性。
* 我们首次证明了，通过整体对抗训练获得的、具有强大防御能力的模型面对黑盒攻击也是脆弱的。

# 2、背景

​		在本节中，我们提供了背景知识，并回顾了对抗性攻击和防御方法的相关工作。给定一个分类器$f(\pmb{x}):\pmb{x}\in\mathcal{\pmb{x}}\rightarrow y\in \mathcal{Y}$，它输出一个标号$y$作为对输入$\pmb{x}$的预测，对抗性攻击的目标是在$\pmb{x}$附近寻找一个样本$\pmb{x}^*$，但是被分类器分类错误。具体来说，有两类对抗样本：非目标和目标。对于一个具有真实标签的、正确分类的输入$\pmb{x}$，比如$f(\pmb{x})=y$，一个非目标对抗样本$\pmb{x}^*$是通过给$\pmb{x}$添加小噪声而不改变标签来制作的，但误导分类器以至于$f(\pmb{x}^*)\neq y$；而一个目标对抗样本旨在通过输出一个特定标签$f(\pmb{x}^*)=y^*$来愚弄分类器，其中$y^*$标签是攻击者指定的目标标签，并且$y^*\neq y$。在大多数情况下，敌方噪声的峰值$L_p$必须小于允许值$\epsilon$，$||\pmb{x}^*-\pmb{x}||_p\le\epsilon$，其中$p$应为$0,1,2,\infin$。

## 2.1、攻击方法

​		现有的生成对抗样本的方法可以分为三类。我们在这里介绍他们的非目标版本的攻击，目标版本可以简单地得出。

​		**基于梯度的单步方法**，如快速梯度符号法（FGSM）[[5](#5)]，通过最大化损失函数$J(\pmb{x}^*,y)$找到一个对抗样本$\pmb{x}^*$，其中$J$通常是交叉熵损失。FGSM通过下面的公式，生成满足$L_\infin$范数$||\pmb{x}^*-\pmb{x}||_\infin \le \epsilon$的对抗样本：
$$
\pmb{x}^*=\pmb{x}+\epsilon \cdot sign(\nabla_{\pmb{x}}J(\pmb{x},y) \tag 1
$$
其中$\nabla_{\pmb{x}}J(\pmb{x},y)$是损失函数关于$\pmb{x}$的梯度。快速梯度法（FGM）是FGSM的一个特例，它满足$L_2$范数界$||\pmb{x}^*-\pmb{x}||_2 \le \epsilon$，即：
$$
\pmb{x}^*=\pmb{x}+\epsilon \cdot \frac{\nabla_{\pmb{x}}J(\pmb{x},y)}{||\nabla_{\pmb{x}}J(\pmb{x},y)||_2} \tag 2
$$
​		**迭代方法**[[9](#9)]以小步长$\alpha$迭代地多次应用快速梯度。 FGSM的迭代版本（I-FGSM）可以表示为：
$$
\pmb{x}_0^*=\pmb{x},\pmb{x}^*_{t+1}=\pmb{x}^*_t+\alpha \cdot sign(\nabla_{\pmb{x}}J(\pmb{x}^*_t,y)) \tag 3
$$
为了使生成的对抗样本满足$L_\infin$（或者$L_2$）界线，可以将$\pmb{x}^*_t$裁剪到$\pmb{x}$的附近（以$\epsilon$为参数），或者简单地设置$\alpha=\epsilon/T$，T为迭代次数。在白盒情况下，迭代方法展现了比单步方法更加强大的对抗性，但是缺点是以可移植性较差为代价[[10](#10)，[24](#24)]。

​		**基于优化的方法**[[23](#23)]直接优化了真实样本与对抗样本之间的距离，但会导致对抗样本分类错误。盒约束的L-BFGS可以用来解决这个问题。一种更复杂的方法[[1](#1)]是解下面这个式子：
$$
\mathop{\arg\min}_{\pmb{x}^*}||\pmb{x}^*-\pmb{x}||_p-J(\pmb{x}^*,y) \tag 4
$$
由于它直接优化了对抗样本与相应真实样本之间的距离，因此无法保证$L_\infin$（$L_2$）距离小于所需值。与迭代方法一样，基于优化的方法也缺乏黑盒攻击的功效。

## 2.2防御方法

​		在许多尝试[[13](#13)、[3](#3)、[15](#15)、[10](#10)、[24](#24)、[17](#17)、[11](#11)]中，对抗训练是提高DNN鲁棒性的、收到最广泛研究的方法[[5](#5)、[10](#10)、[24](#24)]。通过将对抗样本注入训练过程中，经过对抗性训练的模型将学会抵抗损失函数的梯度方向上的扰动。但是，由于生成对抗样本和所训练的参数的耦合，它们没有获得面对黑盒攻击的鲁棒性。集成对抗训练[24](#24)利用不仅仅是受训练模型中，而且还有其他保持模型中产生的对抗样本来扩充训练数据。因此，经过整体对抗训练的模型对于单步攻击和黑盒攻击具有鲁棒性。

# 3、方法论

​		在本文中，我们提出了一系列**基于动量迭代梯度的方法**来生成对抗样本，这些样本可以欺骗白盒模型和黑盒模型。在本节中，我们将详细阐述所提出的算法。我们首先说明如何将动量集成到迭代FGSM中，该方法引入了动量迭代快速梯度符号方法（MI-FGSM），以非目标攻击方式生成满足$L_\infin$范数限制的对抗样本。然后，我们提出了几种有关如何有效攻击模型集成的方法。最后，我们将MI-FGSM扩展到L2范式绑定和有针对性的攻击，从而产生了广泛的攻击方法。

---

**算法 1** MI-FGSM

---

**输入**：一个以$J$为损失函数的分类器$f$ ；真实样本$\pmb{x}$和真实标签$y$；

**输入**:扰动大小$\epsilon$；迭代次数$T$和衰减因子$\mu$。

**输出**:一个满足$||\pmb{x}^*-\pmb{x}||_\infin \le \epsilon$的对抗样本$\pmb{x}^*$。

​	1: $\alpha=\epsilon/T$；

​	2: $\pmb{g}_0=0; \pmb{x}_0^*=\pmb{x}$；

​	3: **for** $t=0$ to $T-1$ **do**

​	4: 		输入$\pmb{x}^*_t$给$f$并且获取梯度$\nabla_{\pmb{x}}J(\pmb{x}^*_t,y)$;

​	5:		 通过累积速度矢量到梯度方向上更新 $\pmb{g}_{t+1}$：
$$
\pmb{g}_{t+1}=\mu \cdot \pmb{g}_t+\frac{\nabla_{\pmb{x}}J(\pmb{x}_t^*,y)}{||\nabla_{\pmb{x}}J(\pmb{x}_t^*,y)||_1}; \tag 6
$$
​	6:		 利用梯度方向更新$\pmb{x}^*_{t+1}$：
$$
\pmb{x}^*_{t+1} = \pmb{x}^*_{t}+\alpha \cdot sign(\pmb{g}_{t+1}); \tag 7
$$


​	7: **end for**

​	8: **return** $\pmb{x}^*=\pmb{x}_T^*$.

---

## 3.1、动量迭代快速梯度符号法

​		动量法[[18](#18)]是一种通过在迭代过程中沿损失函数的梯度方向累积速度矢量来加速梯度下降算法的技术。以前的梯度的记忆有助于穿越狭窄的山谷，小驼峰和较差的局部最小值或最大值[[4](#4)]。动量法还显示了其在随机梯度下降中稳定更新的有效性[[20](#20)]。我们运用动量的思想来产生对抗性的例子并获得巨大的效益。

​		为了从满足$L_\infin$范数界的真实样本$\pmb{x}$生成非目标对抗样本$\pmb{x}^*$，基于梯度的方法通过解决约束优化问题来寻找对抗样本：
$$
\mathop{\arg\max}_{\pmb{x}^*}J(\pmb{x}^*,y),\space s.t. \space ||\pmb{x}^*-\pmb{x}||_\infin\le \epsilon, \tag 5
$$
其中$\epsilon$是对抗性扰动的大小。 FGSM通过在数据点周围决策边界的线性假设下，仅一次将梯度的符号应用于真实样本（在等式（1）中）来生成一个对抗示例。但是在实践中，当扰动较大时，线性假设可能不成立[[12](#12)]，这使得FGSM生成的对抗样本“欠拟合”模型，从而限制了其攻击能力。相反，（在等式（3）中），在每次迭代中，迭代FGSM贪婪地将对抗性示例沿梯度的符号方向移动。因此，对抗样本很容易陷入不良的局部最大值并“过拟合”模型，使其不太可能在模型之间转移。

​		为了克服这种难题，我们将动量整合到迭代FGSM中，以稳定更新方向并摆脱出现的不良的局部最大值。因此，当增加迭代次数时，基于动量的方法仍可保持对抗样本的可传递性，同时，对于像迭代FGSM这样的白盒模型，它也可作为强大的对手。它减轻了攻击能力和可传递性之间的折衷，展现了强大的黑盒攻击能力。

​		算法1中总结了动量迭代快速梯度符号方法（MI-FGSM）。具体而言，$\pmb{g}_t$使用公式（6）中定义的衰减因子$\mu$，收集前$t$次迭代的梯度。然后，在公式（7）中，对抗样本$\pmb{x}_t^*$直到第$t$次迭代在$\pmb{g}_t$的符号方向上被扰动，步长为$\alpha$。如果$\mu$等于0，则MI-FGSM退化为迭代FGSM。在每次迭代中，当前梯度$\nabla_{\pmb{x}}J(\pmb{x}_t^*,y)$通过其自身的$L_1$距离（任何距离度量都是可行的）进行归一化，因为我们注意到不同迭代中梯度的大小有所不同。

## 3.2、攻击模型集合

​		在本节中，我们研究如何有效地攻击模型集合。集成方法已被广泛用于研究和竞赛中，以提高性能和鲁棒性[[6](#6)、[8](#8)、[2](#2)]。集合的思想也可以应用于对抗攻击，因为这样的事实：如果一个样本对多个模型仍然具有对抗性，那么它可能会捕获一个固有的方向，该方向总是欺骗这些模型，并且更有可能同时移植到其他模型[ [12](#12)]，从而启用强大的黑盒攻击。

​		我们准备攻击将logits函数（logits函数是softmax的输入值，一般是全连接层的输出，也可以用来表征概率，只是还没有进行归一化，由于logits没有一个标准的中文名，因此这里仍然沿用其英文，下同）融合在一起的多个模型，并将此方法称为*logits集合*。因为logits捕获了概率预测之间的对数关系，所以由logits融合的模型集合会汇总所有模型的精细详细输出，可以轻松发现其漏洞。特别是，为了攻击K个模型的集合，我们将logits融合为：
$$
\pmb{l}(\pmb{x})=\sum_{k=1}^Kw_k\pmb{l}_k(\pmb{x}) \tag 8
$$
其中$\pmb{l}_k(\pmb{x})$是第$k$个模型的logits，$w_k$是满足$s_k\ge0$且$\sum_{k=1}^Kw_k=1$的整体权重。损失函数$J(\pmb{x},y)$定义为软最大交叉熵损失：
$$
J(\pmb{x},y)=-\pmb{1}_y\cdot log(softmax(\pmb{l}(\pmb{x}))),\tag 9
$$
其中$\pmb{1}_y$是$y$的独热编码。我们总结了MI-FGSM算法，在算法2中用于攻击平均了多个logits的模型。

​		为了进行比较，我们还介绍了两种替代的集成方案，其中一个已经进行了研究[[12](#12)]。具体而言，可以在预测中对$K$个模型平均为[[12](#12)]$\pmb{p}(\pmb{x})=\sum_{k=1}^Kw_k\pmb{p}_k(\pmb{x})$，其中$\pmb{p}_k(\pmb{x})$是给出输入$\pmb{x}$下第$k$个模型的预测概率。$K$个模型的损失也可以平均为$J(\pmb{x},y)=\sum_{k=1}^Kw_kJ_k(x,y)$。

---

**算法 2** 针对模型集合的MI-FGSM

---

**输入：**K个分类器的logits $\pmb{l}_1,\pmb{l}_2,……,\pmb{l}_k$；集合权重$w_1,w_2,……,w_K$；一个真实样本$\pmb{x}$和真实标签$y$；

**输入：**扰动大小$\epsilon$；迭代次数$T$和衰减因子$\mu$。

**输出：**一个对抗样本$\pmb{x}^*$，满足$||\pmb{x}^*-\pmb{x}||_\infin \le \epsilon$。

1: $\alpha=\epsilon /T %$；

2: $\pmb{g}_0=0;\pmb{x}_0^*=\pmb{x}$；

3: **for** $t=0$ to $T-1$ **do**

4: 	输入$\pmb{x}_t^*$并且输出$\pmb{l}(\pmb{x}_t^*)$，对$k=1,2,……,K$；

5: 	融合logits为$\pmb{l}(\pmb{x}^*_t)=\sum_{k=1}^Kw_k\pmb{l}_k(\pmb{x}_t^*)$；

6: 	根据$\pmb{l}(\pmb{x}_t^*)$和公式（9）获取软最大交叉熵损失$J(\pmb{x}^*_t,y)$；

7: 	得到梯度$\nabla_{\pmb{x}}J(\pmb{x}_t^*,y)$；

8: 	根据公式（6）更新$\pmb{g}_{t+1}$；

9: 	根据公式（7）更新$\pmb{x}_{t+1}^*$；

10: **end for**

11: **return** $\pmb{x}^*=\pmb{x}_T^*$。

---

在这三种方法中，唯一的区别是在哪里组合多个模型的输出，但它们会导致不同的攻击能力。我们根据经验经验发现，在集合的各种攻击方法和各种模型中，logits集合在预测和损失方面的性能都优于预测集合和损失集合，这将在4.3.1节中加以说明。

## 3.3、拓展

​		动量迭代方法可以很容易地推广到其他攻击环境。通过用前面所有步骤的累积梯度代替当前梯度，任何迭代方法都可以推广到其动量变体。这里本文从$L_2$范数界限攻击和目标攻击两个方面介绍了生成对抗样本的方法。

​		为了在距离为$||\pmb{x}^*-\pmb{x}||\le\epsilon$的区域样本附近找到对抗样本，迭代快速梯度法（MI-FGM）的动量变量可以写成
$$
\pmb{x}_{t+1}^*=\pmb{x}_t^*+\alpha\cdot\frac{\pmb{g}_{t+1}}{||\pmb{g}_{t+1}||_2},\tag {10}
$$
这里$\pmb{g}_{t+1}$在公式（6）中定义，并且$\alpha=\epsilon /T$，T代表迭代总次数。

​		对于目标攻击，寻找误分类为目标类$y^*$的典型例子的目的是最小化损失函数$J(\pmb{x}^*,y^*)$。累积梯度的计算公式为
$$
\pmb{g}_{t+1}=\mu \cdot \pmb{g}_t+\frac{J(\pmb{x}_t^*,y^*)}{||\nabla_{\pmb{x}}J(\pmb{x}_t^*,y^*)||_1}; \tag{11}
$$
以$L_\infin$范数为界的目标MI_FGSM是：
$$
\pmb{x}_{t+1}^*=\pmb{x}_t^*-\alpha\cdot sign(\pmb{g}_{t+1}) \tag{12}
$$
并且以$L_2$范数为界线的目标MI_FGM是：
$$
\pmb{x}_{t+1}^*=\pmb{x}_t^*-\alpha \cdot \frac{\pmb{g}_{t+1}}{||\pmb{g}_{t+1}||_2}\tag{13}
$$
因此，我们介绍了一系列的动量迭代方法来解决各种情况下的攻击，其有效性在第4节中得到了证明。

# 4、实验

​		在本节中，我们在ImageNet数据集[[19](#19)]上进行了大量实验，以验证所提出方法的有效性。我们首先在4.1节中确定实验设置。 然后我们在4.2节中报告了对单个模型的测试结果，在4.3节中报告了一组模型的测试结果我们的方法赢得了NIPS 2017非目标和目标对抗攻击竞赛，配置介绍见4.4节。

## 4.1、设置

​		我们研究了七个模型，其中四个是正常训练模型（Inc-v3）[[22](#22)]、（Inc-v4）、（IncRes-v2）[[21](#21)]、（Res-152）[[7](#7)]，另外三个模型由集成对抗训练$Inc-v3_{ens3}$、$Inc-v3_{ens4}$、$IncRes-v2_{ens}$[[24](#24)]。我们将最后三个模型简单地称为“对抗训练模型”，没有歧义。

​		如果模型不能对原始图像进行分类，那么研究攻击的成功率就没有什么意义了没错，所以，我们从ILSVRC 2012验证集中随机选择了1000张属于1000个类别的图像，它们都被正确分类。

​		在我们的实验中，我们将我们的方法与单步梯度法和迭代法进行了比较。由于基于优化的方法不能显式地控制对抗样本和相应的真实样本之间的距离，因此它们不能直接与我们的方法相比，但是它们与第2.1节中讨论的迭代方法具有相似的性质。为清晰起见，我们只报告了基于非目标攻击的$L_\infin$范数界的结果，并在补充资料中留下了基于$L_2$范数界和目标攻击的结果。本文的研究结果在不同的攻击环境下具有普遍性。

## 4.2、攻击单个模型

​		我们在表1中报告了我们所考虑的模型的攻击成功率。Inc-v3、Inc-v4、InvRes-v2和Res-152分别采用FGSM、迭代FGSM（I-FGSM）和MI-FGSM攻击方法生成了对抗样本。成功率是以对抗样本为输入的相应模型的误分类率。在所有实验中，最大扰动$\epsilon$被设置为16，像素值在[0，255]中。I-FGSM和MI-FGSM的迭代次数为10次，衰减因子$\mu$为1.0，这将在4.2.1节中研究。

![image-20210112143156342](/Users/tongkangheng/Library/Application Support/typora-user-images/image-20210112143156342.png)
$$
表1。我们研究的七种模型的非目标对抗攻击的成功率（\%）。\\
对抗性示例分别针对Inc-v3、Inc-v4、IncRes-v2和Res-152编制，\\
使用FGSM、I-FGSM和MI-FGSM。*表示白盒攻击。
$$
​		从表中可以看出，MI-FGSM仍然像I-FGSM一样是强大的白盒对手，因为它可以以接近100％的成功率攻击白盒模型。另一方面，可以看出，与单步FGSM相比，I-FGSM降低了黑盒攻击的成功率。但是，通过整合动量，我们的MI-FGSM在黑盒攻击中的性能明显优于FGSM和I-FGSM。在大多数黑盒攻击案例中，它的成功率是I-FGSM的2倍以上，证明了所提算法的有效性。我们在图1中显示了为Inc-v3生成的两个对抗图像。

![image-20210112115335033](/Users/tongkangheng/Library/Application Support/typora-user-images/image-20210112115335033.png)
$$
图1。我们展示了两个由提出的动量迭代快速梯度符号法（MI-FGSM）生成的对抗样本模型。\\
左栏目：原始图像。中间栏目：应用MI-FGSM10次迭代产生的对抗性噪音。右栏目：生成的对抗图像。\\
我们同时显示由Inception-v3对这些图像预测的标签和的概率。
$$
​		应该注意的是，尽管我们的方法大大提高了黑盒攻击的成功率，但是对于以黑盒方式攻击经过对抗训练的模型（例如，对于$IncRes-v2_{ens}$而言不足16％）仍然无效。后来我们证明了基于集成的方法极大地改善了4.3节中的结果。接下来，我们研究MI-FGSM不同于原始迭代方法的几个方面，以进一步解释为什么它在实践中表现良好。

### 4.2.1、衰减因子$\mu$

​		衰减因子$\mu$在提高攻击成功率方面起着关键作用。如果$\mu=0$，则基于动量的迭代方法简单地转向普通的迭代方法。因此，我们研究了适当的衰减因子值。我们用MI-FGSM攻击Inc-v3模型，扰动大小$\epsilon=16$，迭代次数10，衰减因子范围从0.0到2.0，粒度为0.1。图2中针对Inc-v3，Inc-v4，IncRes-v2和Res-152生成的对抗样本的成功率。成功率对黑盒模型的曲线是单峰的，其最大值在$\mu=1.0$左右获得。当$\mu=1.0$时，对等式（6）中定义的$\pmb{g}_t$的另一种解释只是将所有以前的提督加起来以执行当前更新。

![image-20210112115845463](/Users/tongkangheng/Library/Application Support/typora-user-images/image-20210112115845463.png)
$$
图2。Inc-v3、Inc-v3（白盒）、Inc-v4、IncRes-v2和Res-152（黑盒）的对抗样本的成功率（\%），\mu范围为0.0到2.0
$$

### 4.2.2、迭代次数

​		然后，我们研究使用I-FGSM和MI-FGSM时迭代次数对成功率的影响。我们采用相同的超参数（即$\epsilon=16,\mu=1.0$）来攻击Inc-v3模型，迭代次数为1到10，然后评估针对Inc-v3，Inc-v4，IncRes-v2和Res-152模型的对抗样本的成功率，结果如图3所示。

![image-20210112120128207](/Users/tongkangheng/Library/Application Support/typora-user-images/image-20210112120128207.png)
$$
图3.针对Inc-v3模型（白盒），Inc-v4，IncRes-v2和Res-152（黑盒）针对Inc-v3模型生成的对抗示例的成功率（％）。我们将I-FGSM和MI-FGSM的结果与不同的迭代进行比较。请注意，Inc-v3与MI-FGSM的曲线以及Inc-v3与I-FGSM的曲线重叠。
$$
​		可以看到，当增加迭代次数时，I-FGSM相对于黑盒模型的成功率逐渐降低，而MI-FGSM的成功率则保持较高的值。结果证明了我们的论点，即通过迭代方法生成的对抗示例很容易适合白盒模型，但不太可能在模型之间转移。但是基于动量的迭代方法有助于减轻白盒攻击和可传递性之间的折衷，从而同时展示了针对白盒和黑盒模型的强大攻击能力。

### 4.2.3、更新方向

​		为了解释为什么MI-FGSM具有更好的移植能力，我们进一步研究了I-FGSM和MI-FGSM在迭代过程中给出的更新方向。我们计算两个连续扰动的余弦相似度，并在攻击Inc-v3时在图4中显示结果。由于MI-FGSM中的余弦相似度值较大，因此MI-FGSM的更新方向比I-FGSM的更新方向更稳定。

​		回想一下，可移植性来自以下事实：模型在数据点周围学习相似的决策边界[[12](#12)]。尽管决策边界是相似的，但由于DNN的高度非线性结构，它们不可能相同。因此，模型的数据点周围可能存在一些例外的决策区域（[[12](#12)]中的图4和图5所示的漏洞），这些决策区域很难转移到其他模型。这些区域在优化过程中对应于较差的局部最大值，并且迭代方法可以轻松地陷入此类区域，从而导致可移植的对抗样本较少。另一方面，通过图4所示的动量方法获得的稳定更新方向可以帮助从这些例外区域中逃脱，从而为对抗性攻击提供更好的可传递性。另一种解释是，稳定的更新方向使摄动的$L_2$范数更大，这可能对可移植性有所帮助。

![image-20210112140928666](/Users/tongkangheng/Library/Application Support/typora-user-images/image-20210112140928666.png)
$$
图4。在攻击Inc-v3模型时，两个连续扰动在I-FGSM和MI-FGSM的余弦相似性。结果平均超过1000张图像。
$$

### 4.2.4、扰动大小

​		我们最终研究了对抗扰动的大小对成功率的影响。我们使用图像强度[0,255]的FGSM，I-FGSM和MI-FGSM，$\epsilon$从1到40范围攻击Inc-v3模型，并评估在白盒模型Inc-v3和黑盒模型Res-152上的性能。在我们的实验中，我们将I-FGSM和MI-FGSM的步长$\alpha$设置为1，因此迭代次数随扰动的大小线性增长。结果如图5所示。

![image-20210112141156804](/Users/tongkangheng/Library/Application Support/typora-user-images/image-20210112141156804.png)
$$
图5。Inc-v3与Inc-v3（白盒）和Res-152（黑框）对抗样本的成功率（\%）。	\\
我们比较了不同扰动大小的FGSM、I-FGSM和MI-FGSM的结果。\\
Inc-v3对MI-FGSM和Inc-v3对I-FGSM的曲线重叠在一起。
$$
​		对于白盒攻击，迭代方法很快会达到100％的成功率，但是当扰动较大时，单步FGSM的成功率会降低。当扰动较大时，这种现象很大程度上归因于决策边界为线性的不恰当假设[[12](#12)]。对于黑盒攻击，尽管这三种方法的成功率随扰动的大小线性增长，但MI-FGSM的成功率增长更快。换句话说，为了攻击具有成功率的黑盒模型，MI-FGSM可以使用较小的扰动，这在视觉上对于人类来说是无法区分的。

## 4.3 攻击集成模型

​		在本节中，我们显示了攻击一系列模型的实验结果。我们首先比较3.2节中介绍的三种集成方法。然后证明经过对抗训练的模型容易受到我们的黑盒攻击。

### 4.3.1、几种集成模型的比较

​		在本节中，我们将对攻击的整体方法进行比较。我们在研究中包括四个模型，分别是Inc-v3，Inc-v4，IncRes-v2和Res-152。在我们的实验中，我们将一个模型保留为黑盒模型，并分别通过FGSM，I-FGSM和MI-FGSM攻击其他三个模型的集合，以全面比较这三个集合方法的结果，即logits集合，预测集合和损失集合。我们将I-FGSM和MI-FGSM的迭代次数设置为16，将MI-FGSM的迭代次数设置为10，$\mu$设置为1.0，并将集合权重均等。结果示于表2。

![image-20210112143452895](/Users/tongkangheng/Library/Application Support/typora-user-images/image-20210112143452895.png)
$$
表2。三种集成方法的非目标对抗攻击成功率（\%）。我们报告了一个白盒模型和一个保持黑盒目标模型的结果。\\
我们研究了Inc-v3、Inc-v4、IncRes-v2和Res-152四种模型。\\
在每一行中，“-”表示保持模型的名称，并分别由FGSM、I-FGSM和MI-FGSM为其他三个模型的集合生成对抗性示例。
\\logits集成始终优于其他方法。
$$
​		可以看出，在白盒和黑盒攻击中，在所有攻击方法和不同模型中，logits集合均优于预测集合和损失集合。因此，logits集成方案更适合对抗攻击。

​		表2的另一个观察结果是，MI-FGSM生成的对抗样本以很高的成功率移植，从而实现了强大的黑盒攻击。例如，通过攻击集成了Inc-v4，IncRes-v2和Res-152（不包括Inc-V3)的集合，生成的对抗样本可以以87.9％的成功率欺骗Inc-v3。经过训练的模型表明，他们很容易受到这种攻击。

### 4.3.2、攻击经过了对抗训练的模型

​		为了以黑盒的方式攻击经过对抗训练的模型，我们包括了4.1节中引入的所有七个模型。同样，我们保留一个经过对抗训练的模型作为防御目标模型，以黑盒的方式评估性能，然后对其余六个模型进行集成攻击训练，将其对数以相等的合并权重融合在一起。扰动为16，衰减因子$\mu$为1.0。我们将FGSM，I-FGSM和MI-FGSM的结果与20次迭代进行比较。结果如表3所示。

![image-20210112143735481](/Users/tongkangheng/Library/Application Support/typora-user-images/image-20210112143735481.png)
$$
表3。对集成白盒模型和黑盒模型的非目标对抗攻击的成功率（\%）。\\
我们包括七种型号：Inc-v3、Inc-v4、IncRes-v2、Res-152、Inc-v3_{ens3}、Inc-v3{ens4}和IncRes-v2_{ens}。\\
在每一行中，“-”表示保持模型的名称，并为其他六个模型的集合生成对抗样本。
$$
​		可以看出，经过对抗训练的模型也不能有效地防御我们的攻击，这可以通过40%以上的对抗样本欺骗$Inc-v3_{ens4}$。就我们看到的，在ImageNet上训练的最健壮的模型很容易受到黑箱攻击，从而为开发用于学习健壮的深度学习模型的算法带来了新的安全性问题。

## 4.4、竞赛

​		NIPS 2017对抗攻击和防御竞赛分为三个子竞赛，分别是非针对性对抗攻击，针对性对抗攻击和防御对抗攻击。组织者提供了5000张与ImageNet兼容的图像，用于评估攻击和防御提交的图像。对于每一次攻击，都会为每个图像生成一个对抗示例，其干扰大小在4到16（由组织者指定）的范围内，并且所有对抗样本都将贯穿所有防御方法以获取最终分数。通过本文介绍的方法，我们在非针对性攻击和针对性攻击中均获得了第一名。我们将在提交的文件中指定配置。

​		对于非目标攻击（[源码](https://github.com/dongyp13/Non-Targeted-Adversarial-Attacks)），我们使用MI_FGSM来攻击Inc-v3、Inc-v4、IncRes-v2、Res-152、$Inc-v3_{ens3}$、$Inc-v3_{ens4}$、$IncRes-v2_{ens}$和$Inc-v3_{adv}$的集成模型。我们使用了logits集成方案，集成权重：前7个模型是相等的$1/7.25$，$Inc-v3_{adv}$是$0.25/7.25$。迭代次数是10，衰减因子$\mu$是1.0。

​		对于目标攻击（[源码](https://github.com/dongyp13/Targeted-Adversarial-Attacks)），我们为攻击构建了两幅图像。如果扰动大小小于8，我们以$1/3$和$2/3$的集成权重攻击Inc-v3和$Inc-v3_{ens3}$；否则我们以$4/11,1/11,1/11,4/11$和$1/11$的权重攻击Inc-v3、$Inc-v3_{ens3}$、$Inc-v3_{ens4}$、$IncRes-v2_{ens}$和$Inc-v3_{adv}$的集成模型。迭代次数分别是40和20，衰减因子$\mu$是1.0。

# 5、讨论

​		从不同的角度来看，我们认为找到对抗样本是训练模型的一个类似物，而对抗样本的可移植性也是该模型的可推广性的一个类似物。通过使用元视图，我们实际上将给定一组模型作为训练数据来“训练”一个对抗性示例。这样，通过动量和集成方法获得的改进的可移植性是合理的，因为通常通过采用动量优化器或对更多数据进行训练来提高模型的可推广性。而且我们认为，还可以将其他用于增强模型通用性的技巧（例如SGD）也可以纳入对抗性攻击中，以实现更好的可传递性。

# 6、结论

​		在本文中，我们提出了一类基于动量的迭代方法来增强对抗性攻击，这些方法可以有效地欺骗白盒模型和黑盒模型。我们的方法在黑盒方式中始终优于基于梯度的一步法和原生迭代法。我们进行了广泛的实验，以验证所提出方法的有效性，并解释为什么它们在实践中可行。为了进一步提高生成的对抗样本的可移植性，我们建议攻击其logits融合在一起的模型集合。我们表明，通过整体对抗训练获得的模型容易受到我们的黑盒攻击，这为开发更强大的深度学习模型提出了新的安全问题。

# 致谢

​		这项工作得到了中国国家科学基金会（Nos.61620106010、61621136008、61332007、61571261和U1611461），北京自然科学基金（No.L172037），清华天工智能计算研究所和NVIDIA NVAIL计划的支持，部分由微软亚洲研究中心和清华-英特尔联合研究院建立。

# 参考文献

<span id="1">[1]</span>  N. Carlini and D. Wagner. Towards evaluating the robustnessof  neural  networks.   InIEEE  Symposium  on  Security  and Privacy, 2017. 3

<span id="2">[2]</span>  R.  Caruana,  A.  Niculescu-Mizil,  G.  Crew,  and  A.  Ksikes.Ensemble selection from libraries of models. InICML, 2004.4

<span id="3">[3]</span>  Y. Dong, H. Su, J. Zhu, and F. Bao.  Towards interpretabledeep  neural  networks  by  leveraging  adversarial  examples.arXiv preprint arXiv:1708.05493, 2017. 3

<span id="4">[4]</span>  W. Duch and J. Korczak. Optimization and global minimization methods suitable for neural networks.Neural computing surveys, 2:163–212, 1998. 3

<span id="5">[5]</span>  I. J. Goodfellow, J. Shlens, and C. Szegedy.  Explaining and harnessing adversarial examples. InICLR, 2015. 1, 2, 3

<span id="6">[6]</span>  L. K. Hansen and P. Salamon.   Neural network ensembles.IEEE transactions on pattern analysis and machine intelligence, 12(10):993–1001, 1990. 4

<span id="7">[7]</span>  K. He, X. Zhang, S. Ren, and J. Sun.  Identity mappings indeep residual networks. InECCV, 2016. 5

<span id="8">[8]</span>  A. Krogh and J. Vedelsby.  Neural network ensembles, crossvalidation and active learning. InNIPS, 1994. 4

<span id="9">[9]</span>  A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial examples in the physical world.arXiv preprint arXiv:1607.02533,2016. 1, 2, 3

<span id="10">[10]</span>  A. Kurakin, I. Goodfellow, and S. Bengio.  Adversarial machine learning at scale. InICLR, 2017. 1, 2, 3, 8

<span id=11>[11]</span>  Y. Li and Y. Gal.  Dropout inference in bayesian neural networks with alpha-divergences. InICML, 2017. 3

<span id="12">[12]</span>  Y. Liu, X. Chen, C. Liu, and D. Song. Delving into transferable adversarial examples and black-box attacks.  InICLR,2017. 1, 2, 3, 4, 6, 7, 10

<span id="13">[13]</span>  J. H. Metzen, T. Genewein, V. Fischer, and B. Bischoff.  Ondetecting adversarial perturbations. InICLR, 2017. 3

<span id="14">[14]</span>  S.   M.   Moosavi-Dezfooli,    A.   Fawzi,    O.   Fawzi,    andP. Frossard.  Universal adversarial perturbations.  InCVPR,2017. 1

<span id="15">[15]</span>  T. Pang, C. Du, and J. Zhu. Robust deep learning via reverse cross-entropy training and thresholding test.arXiv preprint arXiv:1706.00633, 2017. 3

<span id="16">[16]</span>  N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik,and A. Swami.  Practical black-box attacks against machine learning.  InProceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, 2017. 2

<span id="17">[17]</span>  N.  Papernot,  P.  McDaniel,  X.  Wu,  S.  Jha,  and  A.  Swami.Distillation as a defense to adversarial perturbations against deep neural networks.  In IEEE Symposium on Security andPrivacy, 2016. 3

<span id="18">[18]</span>  B. T. Polyak. Some methods of speeding up the convergence of iteration methods.USSR Computational Mathematics and Mathematical Physics, 4(5):1–17, 1964. 3

<span id="19">[19]</span>  O.  Russakovsky,  J.  Deng,  H.  Su,  J.  Krause,  S.  Satheesh,S.  Ma,  Z.  Huang,  A.  Karpathy,  A.  Khosla,  M.  Bernstein,et  al.Imagenet  large  scale  visual  recognition  challenge.International Journal of Computer Vision, 115(3):211–252,2015. 2, 5

<span id="20">[20]</span>  I. Sutskever,  J.  Martens,  G. Dahl,  and G. Hinton.   On the importance of initialization and momentum in deep learning.InICML, 2013. 3

<span id="21">[21]</span>  C.  Szegedy,  S.  Ioffe,  V.  Vanhoucke,  and  A.  A.  Alemi.Inception-v4,  inception-resnet  and  the  impact  of  residual connections on learning. InAAAI, 2017. 5

<span id="22">[22]</span>  C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.Rethinking the inception architecture for computer vision. InCVPR, 2016. 1, 5

<span id="23">[23]</span>  C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. InICLR, 2014. 1, 3

<span id="24">[24]</span>  F. Tram`er, A. Kurakin, N. Papernot, D. Boneh, and P. Mc-Daniel. Ensemble adversarial training: Attacks and defenses.InICLR, 2018. 2, 3, 59